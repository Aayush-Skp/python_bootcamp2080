{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a clever chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we will create a chatbot using the cosine similarity algorithm.\n",
    "\n",
    "The basic concept of the chatbot is simple. We want to be able to create a chatbot which will recognise certain input and reply it with a response that makes sense. \n",
    "\n",
    "We will first build the chatbot's database, apply our algorithm of choice, and then create a simple interactive dashboard for chat purposes right on Jupyter Notebook!\n",
    "\n",
    "This exercise is inspired by the [great work](https://medium.com/analytics-vidhya/building-a-simple-chatbot-in-python-using-nltk-7c8c8215ac6e) by Parul Pandey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Adding knowledge base to your chatbot\n",
    "\n",
    "Your chatbot's ability to converse and interact depends on the data that you input to it. Therefore, the first step you want to do is to plan your chatbot. You will have to choose the right knowledge base for your particular task. For instance, you would not train your chatbot on movie reviews if you want it to answer questions about sports cars!\n",
    "\n",
    "In this exercise, you will decide what you want your chatbot to have knowledge on, and look for datasets required. \n",
    "\n",
    "To start off, we have included a document called robots.txt which is simply the wikipedia page on chatbots https://en.wikipedia.org/wiki/Chatbot copied and pasted into a text file.  \n",
    "  \n",
    "What questions would you like your chatbot to answer? You could train your chatbot on endangered species, hunger, gender equality, clean energy, or any other topic you would like! Use the tools you have learned from the previous Experiences to collect information, and process it into a corpus of knowledge.  \n",
    "\n",
    "### Task: What chatbot would you want to make? Have a brainstorming session and share your idea with your group!\n",
    "\n",
    "With that, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing libraries\n",
    "\n",
    "In this exercise, we will be requiring the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # to process text data\n",
    "import numpy as np # to represent corpus as arrays\n",
    "import random \n",
    "import string # to process standard python strings\n",
    "from sklearn.metrics.pairwise import cosine_similarity # We will use this later to decide how similar two sentences are\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Remember when you built a function to create a tfidf bag of words in Experience 2? This function does the same thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we begin to process the knowledge base. We will read the text file, convert everything to lowercase and tokenize it. Do you remember what tokenization does?  \n",
    "  \n",
    "There exists pre-trained tokenizers that will help split your document up into tokens. \n",
    "\n",
    "If this is the first time you are using these pre-trained tokenizers, you will have to download it with the command `nltk.download('punkt')`. We will also use a pre-defined model to perform lemmatization. This is downloaded using `nltk.download('wordnet')`. Subsequently, you can comment out the line if you ever have to run the cell again.\n",
    "\n",
    "#### Optional: run these if you do not have the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rocke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rocke\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # first-time use only tokenizer\n",
    "nltk.download('wordnet') # first-time use only Used for the lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Open file\n",
    "\n",
    "We will first load the .txt file using the open() function. Subsequently, .read() is used to load the data into a variable we will call raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './robots.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mWhen you have collected your data, update the variable 'filepath' below with the location of your knowledge base. \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mThe knowledge base should consist of sentences in a text file.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m      5\u001b[0m filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./robots.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m corpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mopen\u001b[39m(filepath,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m,errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m raw_data\u001b[38;5;241m=\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m (raw_data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './robots.txt'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "When you have collected your data, update the variable 'filepath' below with the location of your knowledge base. \n",
    "The knowledge base should consist of sentences in a text file.\n",
    "'''\n",
    "filepath='./robots.txt'\n",
    "corpus=open(filepath,'r',errors = 'ignore')\n",
    "raw_data=corpus.read()\n",
    "print (raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Conversion to lower case\n",
    "\n",
    "We will convert all text to lower case first. Remember to inspect the result once we are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=raw_data.lower()# converts to lowercase\n",
    "print (raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Sentence segmentation\n",
    "\n",
    "We will use the funtion .sent_tokenizer to convert documents into a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw_data)# converts documents to list of sentences \n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You have separated the document into sentences. However, notice that it is difficult to decipher the individual sentence now.\n",
    "\n",
    "### Task: Print every sentence in a new line!\n",
    "\n",
    "This way, it will be easier to read the sentences!\n",
    "\n",
    "Use the link [here](https://www.geeksforgeeks.org/print-lists-in-python-4-different-ways/) to learn how to do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"printing lists in new line\") \n",
    "  \n",
    "print(*sent_tokens, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Word tokenization\n",
    "\n",
    "We will use the funtion .word_tokenizer to convert sentences into a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = nltk.word_tokenize(raw_data)# converts documents to list of words\n",
    "print (word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our tokens!\n",
    "\n",
    "It is useful to inspect our tokens to make sure they look as expected, and to make sure that we have enough information to train our chatbot. Each sentence token can be considered one piece of knowledge as they provide a single piece of information about your knowledge base. The size of your knowledge base will determine how capable your chatbot will be. \n",
    "\n",
    "The code to print the lengths of your sentence and word tokens are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to check the size of your knowledge base. There should be at least 150 sentence tokens, and if possible, up to 1000\n",
    "# to provide enough context for your chatbot.\n",
    "print(len(sent_tokens), len(word_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! we now have an idea of the amount of information passed to our chatbot. If you find that you do not have enough sentence tokens, this is the time to go back out and search for more information!  \n",
    "  \n",
    "#### 1.2.5 Lemmatization \n",
    "\n",
    "We will lemmatize our word tokens using the WordNetLemmatizer that we have downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer() #Initiate lemmer class. WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.6 Normalization\n",
    "\n",
    "We also write functions to remove punctuation since that will not be useful for our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict))) #see previous section 1.2.5 lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to see how these functions work! call the function on the following sentence and print the output. What has our function done to the test sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence='Today was a wonderful day. The sun was shining so brightly and the birds were chirping loudly!'\n",
    "# your code here\n",
    "test_word_tokens = nltk.word_tokenize(test_sentence)# converts documents to list of words\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "LemTokens(test_word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matching topics with cosine similarity\n",
    "\n",
    "Congratulations, you have now converted your knowledge database from text into sentences and tokens! Do you remember what is the next step after this? Computers are good at processing numbers, and therefore, we will next convert our tokens into numbers!\n",
    "\n",
    "To do so, we will revisit the bag of words and tf-idf from acquire stage and experience 2.\n",
    "\n",
    "How does having our document vector help us to create our chatbot?  \n",
    "\n",
    "### 2.1 How would computer finds similarity?\n",
    "\n",
    "Suppose you want to create a chatbot that can read your input, 'considers' what your input is talking about, and then respond with something that makes the most sense. One of the most common ways is by taking the question, and looking for information within the dataset that is very similar to our question. For example, if the questions contains the phrase 'computer failure', we will assume that the answer lies in sentences that contains words similar to 'computer failure' like 'computer crash', or 'hardware failure'. \n",
    "\n",
    "### 2.2 What is [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)?\n",
    "\n",
    "Remember that we will convert our dataset into document vectors? cosine similarity allows us to find similar vectors, and these vectors will be assumed to be similar in meaning. \n",
    "\n",
    "### Let us work through an example below!\n",
    "\n",
    "#### 2.2.1 Dataset\n",
    "\n",
    "Here, we have 3 sentences. The first 2 are our mini knowledge base, while the third one is our sample question. We will process them into document vectors, and then use cosine similarity to see which one of the knowledge base can give us better answer to the question!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us take two sentences to be the knowledge base, and one more which is a question.\n",
    "Sentence_1='John is my father'\n",
    "Sentence_2='Jane is my mother'\n",
    "Question='who is my father'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Vocabulary\n",
    "\n",
    "The vocabulary is a list of all the words that exist in our sentence and questions.   \n",
    "\n",
    "### Task: List down the vocabulary included in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocabulary = ['John', 'is', 'my', 'father', 'Jane', 'mother', 'who']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Bag of words\n",
    "\n",
    "How would we put this into a bag of words? \n",
    "\n",
    "### Task: Construct an array to represent the information from the two sentences above using a bag of words technique.  \n",
    "\n",
    "Hint 1: The array should be of size (3,7), and each number within the array is either 0 or 1 to indicate the presence of that word in the row.  \n",
    "Hint 2: An array is constructed using the function array_name = np.array([[row 1],[row 2],[row 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag_of_words = # your code here\n",
    "bag_of_words = np.array([[1, 1, 1, 1, 0, 0, 0], \n",
    "                         [0, 1, 1, 0, 1, 1, 0], \n",
    "                         [0, 1, 1, 1, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your bag of words, the first 2 rows encode all your knowledge that is contained in sentence 1 and sentence 2. The third row encodes your question.  \n",
    "To find the piece of knowledge closest to your question, we simply look for the sentence with the highest cosine similarity to your question.  \n",
    "\n",
    "#### 2.2.4 Finding cosine similarity\n",
    "\n",
    "Remember at the beginning of the exercise when we imported all our libraries? One of the libraries was called `from sklearn.metrics.pairwise import cosine_similarity`. The cosine_similarity function allows us to compare sentences. Imagine each sentence as a vector, which is a line pointing in some direction. Cosine similarity calculates the angles between each line and the more similar two lines are, the smaller their angle, and the higher their cosine similarity.  \n",
    "  \n",
    "Let us calculate the cosine similarities between each piece of information in our knowledge base, and the question. Remember the first row of our array corresponds to sentence 1, and the second row corresponds to sentence 2. Let us see which sentence has a higher cosine similarity to the question!\n",
    "\n",
    "### Task: Use indexing function to select only the top 2 rows of bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "bag_of_words = np.array([[1, 1, 1, 1, 0, 0, 0], \n",
    "                         [0, 1, 1, 0, 1, 1, 0], \n",
    "                         [0, 1, 1, 1, 0, 0, 1]])\n",
    "\"\"\"\n",
    "bag_of_words[:2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, you have your database selected, now we want to list the question which is at the last row of the variable bag_of_words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (bag_of_words[-1,:].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print (bag_of_words[-1,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between the 2 output above? Try using the lines for the cosine simililarity exercise below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to apply the cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember our database\n",
    "# Sentence_1='John is my father'\n",
    "# Sentence_2='Jane is my mother'\n",
    "# Question='who is my father'\n",
    "\n",
    "cosine_similarity(bag_of_words[:2,:], bag_of_words[-1,:].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, analyze the result!\n",
    "\n",
    "Which sentence had a higher cosine similarity, and was it the answer you expected? \n",
    "\n",
    "If you got your chatbot to return the sentence with the highest cosine similarity score as the answer, would it have answered the question correctly? That is basically how a chatbot answers questions!  \n",
    "  \n",
    "### Task: Now, can you think of a question which would cause sentence 2 to have a higher cosine similarity? Show us the score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Suggest an update to the initial sentences and the resulting bag of words.\n",
    "# Suggested update to bag of words. This would correspond to the question: ‘Who is my mother?’\n",
    "bag_of_words[-1,:] = [0, 1, 1, 0, 0, 1, 1]\n",
    "cosine_similarity(bag_of_words[:2,:], bag_of_words[-1,:].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now that your chatbot has brains, let us give it a mouth!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get your chatbot to say its first words\n",
    "\n",
    "### 3.1 Greetings\n",
    "At the start of every conversation, your chatbot may expect a greeting. These greetings are not a question, but your chatbot should have a reply to the greeting too. We can input some common greetings you expect to receive, and get your chatbot to reply with a random selection of greeting responses.  \n",
    "   \n",
    "Tip: Your chatbot may throw up some warnings. Do not worry about those as long as your chatbot is giving you reasonable answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Create list of inputs and responses\n",
    "\n",
    "Let's first create the list of greetings your chatbot will have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = [\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\", \"hey there\"]\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Add some greeting inputs and responses to personalize your chatbot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Create function to receive and return greetings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greeting(sentence):\n",
    "    for word in sentence.split(): # Looks at each word in your sentence\n",
    "        if word.lower() in GREETING_INPUTS: # checks if the word matches a GREETING_INPUT\n",
    "            return random.choice(GREETING_RESPONSES) # replies with a GREETING_RESPONSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this out! type different types of greetings into the chatbot and see how it responds. Run the greeting a few different times. Do you get the same answer each time?  \n",
    "Hint: pressing ctrl-enter allows you to run the highlighted cell. Holding ctrl and pressing enter multiple times allows you to run the same cell repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Create function to receive questions and return answers\n",
    "\n",
    "Now let us define a function to calculate a response when someone asks the robot a question. \n",
    "\n",
    "The response function:\n",
    "1. Takes in a question\n",
    "2. Uses cosine similarity to find the closest sentence to the question\n",
    "3. Returns that sentence as an answer\n",
    "\n",
    "To prevent the chatbot from returning completely useless answers, we will only return an answer if it has a cosine similarity greater than 0. Otherise, the chatbot will simply say that it does not understand the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    \n",
    "    robo_response='' # initialize a variable to contain string\n",
    "    sent_tokens.append(user_response) #add user response to sent_tokens\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english') \n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens) #get tfidf value\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf) #get cosine similarity value\n",
    "    idx=vals.argsort()[0][-2] \n",
    "    flat = vals.flatten() \n",
    "    flat.sort() #sort in ascending order\n",
    "    req_tfidf = flat[-2] \n",
    "    \n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Testing the response function\n",
    "\n",
    "Test the response function. Write some questions into the response() function and inspect the results. Are they close to the answers? \n",
    "\n",
    "If you constantly get the response 'I'm sorry! I don't understand you', you might need to add more information into your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response('what do chatbots do')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.5 Adding more information into knowledge base\n",
    "\n",
    "You can add more information into your knowledge base either by hand, for specific answers you have, or by searching for and adding more data into the text file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test your clever chatbot!\n",
    "\n",
    "#### 4.1.1 Testing your chatbot\n",
    "Now it is time to test your chatbot. This chatbot will be run entirely inside your jupyter notebook. You can train your chatbot to do specific tasks on commands. \n",
    "\n",
    "For instance:\n",
    "1. Saying 'bye' will cause the chatbot to shutdown. \n",
    "2. Giving a response that is one of the greeting phrases will cause the chatbot to give a greeting in return.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add more features to your chatbot based on a list of keywords that triggers a certain behavior. For example, you can get your chatbot to tell the time, to tell a joke, or even print pictures.\n",
    "  \n",
    "Your basic chatbot is written below. You can play with it to see how well it can understand questions in your knowledge base. It will not be able to answer questions very well because chatbots require many thousands of sentences which takes many days to train. It should still be able to answer simple questions!  \n",
    "\n",
    "#### 4.1.2 Adding time features to your chatbot\n",
    "\n",
    "You will now add more features to your chatbot to improve its capabilities! For a start, add a feature to your chatbot that will allow it to tell you the time if the user inputs 'time'.  \n",
    "Hint: Use a similar template to the greeting function above. You can get the current time by importing datetime and calling datetime.datetime.now()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def tell_time(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() == 'time':\n",
    "            currentdt = datetime.datetime.now()\n",
    "            return currentdt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "tell_time('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your chatbot now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "                # Uncomment the statement below once you have written your tell_time fuction.\n",
    "#             if(tell_time(user_response)!=None):\n",
    "#                 print(\"ROBO: \"+tell_time(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a very basic chatbot that can be trained on data you have chosen. Improve your chatbot in the following ways:\n",
    "1. increase the number of greeting inputs and greeting responses\n",
    "2. increase the number of words to say goodbye i.e. see you!, quit, exit\n",
    "3. train your chatbot on a different topic, or add to your chatbot's knowledge base by adding more data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
